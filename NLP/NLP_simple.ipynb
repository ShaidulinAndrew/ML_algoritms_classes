{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387e6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмём исходный текст для анализа\n",
    "corpus = 'When we were in Paris we visited a lot of museums. We first went to the Louvre, the largest art museum in the world. I have always been interested in art so I spent many hours there. The museum is enourmous, so a week there would not be enough.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6158f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем основную библиотеку для работы с текстом\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26e3d",
   "metadata": {},
   "source": [
    "Предварительная обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dba09",
   "metadata": {},
   "source": [
    "Шаг 1. Разделение на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1954614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['When we were in Paris we visited a lot of museums.',\n",
       " 'We first went to the Louvre, the largest art museum in the world.',\n",
       " 'I have always been interested in art so I spent many hours there.',\n",
       " 'The museum is enourmous, so a week there would not be enough.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импортируем метод sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# скачиваем модель которая будет делить на предложения\n",
    "nltk.download('punkt')\n",
    "print('')\n",
    "\n",
    "# и применяем метод к нашему тексту\n",
    "sentences = sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b261941",
   "metadata": {},
   "source": [
    "Шаг 2. Разделение на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05734c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'we', 'were', 'in', 'Paris', 'we', 'visited', 'a', 'lot', 'of', 'museums', '.']\n"
     ]
    }
   ],
   "source": [
    "# импортируем метод word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# разобъем на слова первое предложение\n",
    "print(word_tokenize(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab6f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'we', 'were', 'in', 'Paris', 'we', 'visited', 'a', 'lot', 'of', 'museums', '.', 'We', 'first', 'went', 'to', 'the', 'Louvre', ',', 'the', 'largest', 'art', 'museum', 'in', 'the', 'world', '.', 'I', 'have', 'always', 'been', 'interested', 'in', 'art', 'so', 'I', 'spent', 'many', 'hours', 'there', '.', 'The', 'museum', 'is', 'enourmous', ',', 'so', 'a', 'week', 'there', 'would', 'not', 'be', 'enough', '.']\n"
     ]
    }
   ],
   "source": [
    "# теперь проделаем это со всеми предложениями\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    t = word_tokenize(sentence)\n",
    "    tokens.extend(t)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863699a",
   "metadata": {},
   "source": [
    "Шаг 3. Перевод в нижний регистр, удаление стоп-слов и знаков пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1491b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['paris', 'visited', 'lot', 'museums', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hours', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# импортируем модуль стоп-слов\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# скачаем словарь стоп слов\n",
    "nltk.download('stopwords')\n",
    "print('')\n",
    "\n",
    "# используем set, чтобы оставить только уникальные значения\n",
    "unique_stops = set(stopwords.words('english'))\n",
    "\n",
    "# создаем пустой список без стоп-слов\n",
    "no_stops = []\n",
    "\n",
    "for token in tokens:\n",
    "    token = token.lower()\n",
    "    # если токен не в списке стоп-слов и не является знаком пунктуации\n",
    "    if token not in unique_stops and token.isalpha():\n",
    "        no_stops.append(token)\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9b951-e90e-4217-9b72-0be623865e67",
   "metadata": {},
   "source": [
    "__Лемматизация__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3b431e-9de7-433b-a38d-06e8e55f6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'visited', 'lot', 'museum', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hour', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
     ]
    }
   ],
   "source": [
    "# импортируем класс дл лемматизации\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# импортируем словарь\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# создаем объект этого класса\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# и пустой список для слов после лемматизации\n",
    "lemmatized = []\n",
    "\n",
    "# проходимся по всем токенам\n",
    "for token in no_stops:\n",
    "    token = lemmatizer.lemmatize(token)\n",
    "    lemmatized.append(token)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c522d18-330d-45d8-a609-6ecbb3ea0082",
   "metadata": {},
   "source": [
    "__Стемминг (поиск основы слова)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a756ee81-98a1-47a7-bd4f-fd429e305fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pari', 'visit', 'lot', 'museum', 'first', 'went', 'louvr', 'largest', 'art', 'museum', 'world', 'alway', 'interest', 'art', 'spent', 'mani', 'hour', 'museum', 'enourm', 'week', 'would', 'enough']\n"
     ]
    }
   ],
   "source": [
    "# импортируем класс стеммера Porter и создаём объект этого класса\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# используем list comprehension вместо цикла for для стемминга и создание нового списка\n",
    "stemmed_p = [porter.stem(s) for s in lemmatized]\n",
    "print(stemmed_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6570a1a-dbb2-441a-9ada-807371633a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['par', 'visit', 'lot', 'muse', 'first', 'went', 'louvr', 'largest', 'art', 'muse', 'world', 'alway', 'interest', 'art', 'spent', 'many', 'hour', 'muse', 'enourm', 'week', 'would', 'enough']\n"
     ]
    }
   ],
   "source": [
    "# аналогично импортируем класс Lancaster и создаем объект этого класса\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# так же используем list comprehension вместо цикла for для стемминга и создание нового списка\n",
    "stemmed_l = [lancaster.stem(s) for s in lemmatized]\n",
    "print(stemmed_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61744e0c-bf60-4df3-8fdf-8d0e7ce76526",
   "metadata": {},
   "source": [
    "__Мешок слов (bag of words, bow)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb833e-fee0-4f35-be39-4d273f4264c3",
   "metadata": {},
   "source": [
    "С помощью Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8779f51a-bb8f-45af-b54e-2da9809d90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('museum', 3), ('art', 2), ('paris', 1), ('visited', 1), ('lot', 1), ('first', 1), ('went', 1), ('louvre', 1), ('largest', 1), ('world', 1)]\n"
     ]
    }
   ],
   "source": [
    "# из модуля collections импортируем класс Counter\n",
    "from collections import Counter\n",
    "\n",
    "# применяем класс Counter к словам после лемматизации\n",
    "# на выходе нам возвращается словарь {слово: его частота в тексте}\n",
    "bow_counter = Counter(lemmatized)\n",
    "\n",
    "# функция most_common() упорядочивает словарь по значению\n",
    "print(bow_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b35dbb-5bca-4020-9b9f-52194a1cc441",
   "metadata": {},
   "source": [
    "С помощью CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f598153-3c38-4ed2-9e24-0ad0af5ebe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем соответствующий класс\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# создаем объект этого класса и указываем, что хотим перевести слова в нижний регистр\n",
    "# а также отфильтровать стоп слова через stop_words = {'english'}\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             lowercase = True,\n",
    "                             tokenizer = None,\n",
    "                             stop_words = 'english',\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b52519-a084-46ee-829b-6e79f6aeda97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Применяем этот объект к предложениям (документам)\n",
    "bow_cv = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# на выходе получается матрица csr\n",
    "print(type(bow_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120d5c9c-aa5a-4f41-b508-105b5c5cf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 1 0 1 1 0 0 0 0 0 1 1]\n",
      " [1 0 1 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# преобразуем матрицу csr в привычный формат массива Numpy\n",
    "# для этого можно использовать toarray () или todense()\n",
    "\n",
    "print(bow_cv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6042b8-1b7a-42e8-a566-044e97a29bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# строки это предложения (документы), столбцы - слова (токены)\n",
    "bow_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd53b34-64bb-4e89-9bc3-0e1367a7a67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paris': 9, 'visited': 11, 'lot': 5, 'museums': 8, 'went': 13, 'louvre': 6, 'largest': 4, 'art': 0, 'museum': 7, 'world': 14, 'interested': 3, 'spent': 10, 'hours': 2, 'enourmous': 1, 'week': 12}\n",
      "['art' 'enourmous' 'hours' 'interested' 'largest' 'lot' 'louvre' 'museum'\n",
      " 'museums' 'paris' 'spent' 'visited' 'week' 'went' 'world']\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на используемые токены (слова)\n",
    "# здесь числа не частотность, а просто порядковый номер (индекс)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(vocab)\n",
    "\n",
    "# можно вывести слова без индекса\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9c0458-13b1-49de-bf23-d3995fab4b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art</th>\n",
       "      <th>enourmous</th>\n",
       "      <th>hours</th>\n",
       "      <th>interested</th>\n",
       "      <th>largest</th>\n",
       "      <th>lot</th>\n",
       "      <th>louvre</th>\n",
       "      <th>museum</th>\n",
       "      <th>museums</th>\n",
       "      <th>paris</th>\n",
       "      <th>spent</th>\n",
       "      <th>visited</th>\n",
       "      <th>week</th>\n",
       "      <th>went</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            art  enourmous  hours  interested  largest  lot  louvre  museum  \\\n",
       "Sentence_0    0          0      0           0        0    1       0       0   \n",
       "Sentence_1    1          0      0           0        1    0       1       1   \n",
       "Sentence_2    1          0      1           1        0    0       0       0   \n",
       "Sentence_3    0          1      0           0        0    0       0       1   \n",
       "\n",
       "            museums  paris  spent  visited  week  went  world  \n",
       "Sentence_0        1      1      0        1     0     0      0  \n",
       "Sentence_1        0      0      0        0     0     1      1  \n",
       "Sentence_2        0      0      1        0     0     0      0  \n",
       "Sentence_3        0      0      0        0     1     0      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для удобства визуализации преобразуем матрицу в дата фрейм\n",
    "\n",
    "index_list = []\n",
    "for i, _ in enumerate(bow_cv):\n",
    "    index_list.append(f'Sentence_{i}')\n",
    "\n",
    "bow_cv_df = pd.DataFrame(data = bow_cv.toarray(),\n",
    "                         index = index_list,\n",
    "                         columns = tokens)\n",
    "bow_cv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12346dee-bc32-44e4-b24d-10024ad2e17d",
   "metadata": {},
   "source": [
    "__TF-IDF__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ee07b-d0c2-4253-941d-4198e39d36c1",
   "metadata": {},
   "source": [
    "Способ 1. CountVectorizer + TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56673094-b3a7-47d8-b4c2-38e8222fb9ad",
   "metadata": {},
   "source": [
    "1) Расчет TF, term frequency, частоты слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ba1d2d-bf19-407d-b73c-a6af133f58e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# этот шаг мы уже сделали\n",
    "bow_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445597ec-3f9e-442d-90ca-14fa5045b626",
   "metadata": {},
   "source": [
    "2) Теперь нужно расчитать IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0339c954-e28d-49ca-9b82-d3627f9988fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art</th>\n",
       "      <th>enourmous</th>\n",
       "      <th>hours</th>\n",
       "      <th>interested</th>\n",
       "      <th>largest</th>\n",
       "      <th>lot</th>\n",
       "      <th>louvre</th>\n",
       "      <th>museum</th>\n",
       "      <th>museums</th>\n",
       "      <th>paris</th>\n",
       "      <th>spent</th>\n",
       "      <th>visited</th>\n",
       "      <th>week</th>\n",
       "      <th>went</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idf_weights</th>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  art  enourmous     hours  interested   largest       lot  \\\n",
       "idf_weights  1.510826   1.916291  1.916291    1.916291  1.916291  1.916291   \n",
       "\n",
       "               louvre    museum   museums     paris     spent   visited  \\\n",
       "idf_weights  1.916291  1.510826  1.916291  1.916291  1.916291  1.916291   \n",
       "\n",
       "                 week      went     world  \n",
       "idf_weights  1.916291  1.916291  1.916291  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортируем TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# создадим объект класса TfidfTransformer \n",
    "tfidf_trans = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "\n",
    "# расчитаем IDF слов\n",
    "tfidf_trans.fit(bow_cv) \n",
    "\n",
    "# поместим результат в датафрейм\n",
    "df_idf = pd.DataFrame(tfidf_trans.idf_, index = tokens, columns = ['idf_weights'])\n",
    "df_idf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88255cde-e8ed-49b6-a0b3-f1b0afe86955",
   "metadata": {},
   "source": [
    "3) Остаётся TF x IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b52f4a5-d6f4-4852-bc93-ad3f1b206637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x15 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# расчитаем TF x IDF\n",
    "tf_idf_vector = tfidf_trans.transform(bow_cv) \n",
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "174a2c4d-9733-4b7d-95c5-e9eeb6f59e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3\n",
      "art         0.0  0.344315  0.414289  0.000000\n",
      "enourmous   0.0  0.000000  0.000000  0.617614\n",
      "hours       0.0  0.000000  0.525473  0.000000\n",
      "interested  0.0  0.000000  0.525473  0.000000\n",
      "largest     0.0  0.436719  0.000000  0.000000\n",
      "lot         0.5  0.000000  0.000000  0.000000\n",
      "louvre      0.0  0.436719  0.000000  0.000000\n",
      "museum      0.0  0.344315  0.000000  0.486934\n",
      "museums     0.5  0.000000  0.000000  0.000000\n",
      "paris       0.5  0.000000  0.000000  0.000000\n",
      "spent       0.0  0.000000  0.525473  0.000000\n",
      "visited     0.5  0.000000  0.000000  0.000000\n",
      "week        0.0  0.000000  0.000000  0.617614\n",
      "went        0.0  0.436719  0.000000  0.000000\n",
      "world       0.0  0.436719  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# теперь мы можем посмотреть на показатель TF-IDF для конкретного слова в конкретном документе\n",
    "\n",
    "# для этого переведем матрицу csr в обычный массив Numpy\n",
    "df_tfidf = pd.DataFrame(tf_idf_vector.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "# и транспонируем его\n",
    "print(df_tfidf.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb286d-90e5-4b77-8b92-975526a98f16",
   "metadata": {},
   "source": [
    "Способ 2. TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dacea23-42b1-4aef-871f-d01055045074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4477d04-a5f7-4368-9fe7-cf56685853fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x15 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем объект класса TfidfVectorizer\n",
    "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words= 'english')\n",
    " \n",
    "# сразу рассчитываем TF-IDF слов\n",
    "tfIdf = tfIdfVectorizer.fit_transform(sentences)\n",
    "tfIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e995de-ac3b-4e97-86ef-7dc2dd13cb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['art' 'enourmous' 'hours' 'interested' 'largest' 'lot' 'louvre' 'museum'\n",
      " 'museums' 'paris' 'spent' 'visited' 'week' 'went' 'world']\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на результат фильтрации\n",
    "print(tfIdfVectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc55a4b-f1c1-4156-bc2c-a3006b94330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51082562, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.51082562, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим IDF слов\n",
    "tfIdfVectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e33d72-d31d-40d1-92cf-c092a46bf8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# количество предложений (документов) х количество слов\n",
    "tfIdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb73e9-7a2a-4912-b6bc-d3ca2ae8de64",
   "metadata": {},
   "source": [
    "Расчет значения TF-IDF для каждого слова по каждому тексту "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "978431e1-88dd-4e2c-935a-ece87be22f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3\n",
      "art         0.0  0.344315  0.414289  0.000000\n",
      "enourmous   0.0  0.000000  0.000000  0.617614\n",
      "hours       0.0  0.000000  0.525473  0.000000\n",
      "interested  0.0  0.000000  0.525473  0.000000\n",
      "largest     0.0  0.436719  0.000000  0.000000\n",
      "lot         0.5  0.000000  0.000000  0.000000\n",
      "louvre      0.0  0.436719  0.000000  0.000000\n",
      "museum      0.0  0.344315  0.000000  0.486934\n",
      "museums     0.5  0.000000  0.000000  0.000000\n",
      "paris       0.5  0.000000  0.000000  0.000000\n",
      "spent       0.0  0.000000  0.525473  0.000000\n",
      "visited     0.5  0.000000  0.000000  0.000000\n",
      "week        0.0  0.000000  0.000000  0.617614\n",
      "went        0.0  0.436719  0.000000  0.000000\n",
      "world       0.0  0.436719  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# чем значение уникальнее для конкретного документа, тым выше показатель\n",
    "df_tfidf = pd.DataFrame(tfIdf.toarray(), columns = tfIdfVectorizer.get_feature_names_out())\n",
    "print(df_tfidf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f53660e-0687-4d2b-b78b-769f7f2807bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.18965082, 0.15440359, 0.13136819, 0.13136819, 0.10917983,\n",
       "         0.125     , 0.10917983, 0.2078122 , 0.125     , 0.125     ,\n",
       "         0.13136819, 0.125     , 0.15440359, 0.10917983, 0.10917983]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# рассчитаем среднее арифметическое по строкам (axis = 0)\n",
    "tfIdf.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad9fd206-297b-4984-873a-ce953cd2d816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18965082, 0.15440359, 0.13136819, 0.13136819, 0.10917983,\n",
       "        0.125     , 0.10917983, 0.2078122 , 0.125     , 0.125     ,\n",
       "        0.13136819, 0.125     , 0.15440359, 0.10917983, 0.10917983]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобразуем матрицу в массив Numpy\n",
    "np.asarray(tfIdf.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35812e2-1be3-44bf-bd47-9f5b1ab3627b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим сколько измерений\n",
    "np.asarray(tfIdf.mean(axis = 0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6957ade4-e4a6-49b1-8007-9065243b7883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18965082, 0.15440359, 0.13136819, 0.13136819, 0.10917983,\n",
       "       0.125     , 0.10917983, 0.2078122 , 0.125     , 0.125     ,\n",
       "       0.13136819, 0.125     , 0.15440359, 0.10917983, 0.10917983])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уберем второе измерение массива\n",
    "np.asarray(tfIdf.mean(axis = 0)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba362d37-04cf-4870-a816-63bd8fb77f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим сколько измерений\n",
    "np.asarray(tfIdf.mean(axis = 0)).ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "988ae344-adff-40de-b27c-1bc45f200ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18965081782108964, 0.15440359274390048, 0.13136818731601646, 0.13136818731601646, 0.10917982746877804, 0.125, 0.10917982746877804, 0.2078121960479979, 0.125, 0.125, 0.13136818731601646, 0.125, 0.15440359274390048, 0.10917982746877804, 0.10917982746877804]\n"
     ]
    }
   ],
   "source": [
    "# преобразуем в список\n",
    "mean_weight = np.asarray(tfIdf.mean(axis = 0)).ravel().tolist()\n",
    "print(mean_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "311c9982-6b34-4679-b305-752d7a8c5bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>mean_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>museum</td>\n",
       "      <td>0.207812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>0.189651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enourmous</td>\n",
       "      <td>0.154404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>week</td>\n",
       "      <td>0.154404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hours</td>\n",
       "      <td>0.131368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interested</td>\n",
       "      <td>0.131368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spent</td>\n",
       "      <td>0.131368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lot</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>museums</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paris</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  mean_weight\n",
       "0      museum     0.207812\n",
       "1         art     0.189651\n",
       "2   enourmous     0.154404\n",
       "3        week     0.154404\n",
       "4       hours     0.131368\n",
       "5  interested     0.131368\n",
       "6       spent     0.131368\n",
       "7         lot     0.125000\n",
       "8     museums     0.125000\n",
       "9       paris     0.125000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем датафрейм из словаря\n",
    "mean_weight_df = pd.DataFrame({'term': tfIdfVectorizer.get_feature_names_out(), \n",
    "                               'mean_weight': mean_weight})\n",
    "\n",
    "# сортируем по убыванию 10 слов с максимальным средним TF-IDF\n",
    "mean_weight_df.sort_values(by = 'mean_weight', ascending = False).reset_index(drop = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe012b2-1839-4336-b87f-c54fa8d45594",
   "metadata": {},
   "source": [
    "Косинусное расстояние между текстовыми векторами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "623f0a59-b45c-479a-b604-8617ad4848a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.4261596  0.4261596  0.4261596  0.4261596  0.\n",
      "  0.4261596  0.30321606]\n",
      " [0.6316672  0.         0.         0.         0.         0.6316672\n",
      "  0.         0.44943642]]\n"
     ]
    }
   ],
   "source": [
    "# возьмем для простоты два текста (предложения)\n",
    "text1 = 'all the world`s a stage, and all the men and women merely players'\n",
    "text2 = 'you must be the change you wish to see in the world'\n",
    "\n",
    "# объеденим им в корпус\n",
    "corpus = [text1, text2]\n",
    "\n",
    "# создадим объект класса TfidfVectorizer\n",
    "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n",
    "\n",
    "# на выходе получаем два вектора, где каждое значение - это вес слова\n",
    "X = tfIdfVectorizer.fit_transform(corpus)\n",
    "\n",
    "# преобразуем данные в формат массива Numpy\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed862a4f-87f1-4a0b-b549-7b8e06b18994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e945c63e-cee0-48af-9617-668ae8d68bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>change</th>\n",
       "      <th>men</th>\n",
       "      <th>merely</th>\n",
       "      <th>players</th>\n",
       "      <th>stage</th>\n",
       "      <th>wish</th>\n",
       "      <th>women</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vector1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.303216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector2</th>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.449436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           change      men   merely  players    stage      wish    women  \\\n",
       "vector1  0.000000  0.42616  0.42616  0.42616  0.42616  0.000000  0.42616   \n",
       "vector2  0.631667  0.00000  0.00000  0.00000  0.00000  0.631667  0.00000   \n",
       "\n",
       "            world  \n",
       "vector1  0.303216  \n",
       "vector2  0.449436  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для удобства можем посмотреть на веса в формате датафрейма\n",
    "vectors_df = pd.DataFrame(data = X.toarray(),\n",
    "                          index = ['vector1', 'vector2'],\n",
    "                          columns = tfIdfVectorizer.get_feature_names_out())\n",
    "vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "206b6ffd-338a-402a-8571-8b0de61b4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем вектора по отдельности\n",
    "vector1 = X.toarray()[0]\n",
    "vector2 = X.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51797e28-f483-4b66-9909-2dbb7dda7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вначале выполним операции в числителе формулы\n",
    "numerator = np.dot(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85c8d9b3-b2b2-4447-9fc3-128426923774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь займемся знаменателем и\n",
    "# (1) рассчитаем длины (по большому счету, это теорема Пифагора)\n",
    "vector1Len = np.linalg.norm(vector1)\n",
    "vector2Len = np.linalg.norm(vector2)\n",
    "\n",
    "# (2) перемножим их\n",
    "denominator = vector1Len * vector2Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba3693bb-d4f5-4ae1-94fc-9a0f886138e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13627634143908643"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# помотрим чему равен косинус угла между векторами\n",
    "cosine = numerator/denominator\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba69c587-cab1-42ce-8482-f4be5fc7ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.17"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# найдем угол в градусах по его косинусу\n",
    "# вычислим угол в радианах\n",
    "angle_radians = np.arccos(cosine)\n",
    "\n",
    "# затем в градусах\n",
    "angle_degrees = angle_radians * 360/2/np.pi\n",
    "round(angle_degrees, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683e7ac-3f51-4933-a84c-b3aa1e1bfd59",
   "metadata": {},
   "source": [
    "Кластерный анализ текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a62075d-d410-4240-bda8-569f9d4e9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в тексте ниже две темы: наука о данных и Большой театр (Википедия)\n",
    "text = '''\n",
    "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data.\n",
    "It applies knowledge and actionable insights from data across a broad range of application domains.\n",
    "Data science is related to data mining, machine learning and big data.\n",
    "The Bolshoi Theatre is a historic theatre in Moscow, Russia.\n",
    "It was originally designed by architect Joseph Bové, which holds ballet and opera performances.\n",
    "Before the October Revolution it was a part of the Imperial Theatres of the Russian Empire along with Maly Theatre in Moscow and a few theatres in Saint Petersburg.\n",
    "Data science is a concept to unify statistics, data analysis, informatics, and their related methods in order to understand and analyze actual phenomena with data.\n",
    "However, data science is different from computer science and information science.\n",
    "The main building of the theatre, rebuilt and renovated several times during its history, is a landmark of Moscow and Russia.\n",
    "On 28 October 2011, the Bolshoi re-opened after an extensive six-year renovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4597a60-091a-475f-bd93-743654f58c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим список из предложений\n",
    "corpus = []\n",
    "\n",
    "# для этого в цикле for пройдемся по тексту, разделяя его по символу новой строки \\n\n",
    "for line in text.split('\\n'):\n",
    "    if line:\n",
    "        line = line.lower()\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "141c541d-ea3f-461b-8ebe-1dd1c5c68450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data.',\n",
       " 'it applies knowledge and actionable insights from data across a broad range of application domains.',\n",
       " 'data science is related to data mining, machine learning and big data.',\n",
       " 'the bolshoi theatre is a historic theatre in moscow, russia.',\n",
       " 'it was originally designed by architect joseph bové, which holds ballet and opera performances.',\n",
       " 'before the october revolution it was a part of the imperial theatres of the russian empire along with maly theatre in moscow and a few theatres in saint petersburg.',\n",
       " 'data science is a concept to unify statistics, data analysis, informatics, and their related methods in order to understand and analyze actual phenomena with data.',\n",
       " 'however, data science is different from computer science and information science.',\n",
       " 'the main building of the theatre, rebuilt and renovated several times during its history, is a landmark of moscow and russia.',\n",
       " 'on 28 october 2011, the bolshoi re-opened after an extensive six-year renovation.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcc055bf-66df-4173-b3e8-25b6e7b56969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим TfidfVectorizer\n",
    "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n",
    "\n",
    "# на выжоде получаем векторы предложений\n",
    "X = tfIdfVectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98fc3896-1127-48e7-aa09-0095185fe1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# импортируем алгоритм k-средних из библиотеки sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# так как мы знаем, что темы две, используем гиперпараметр k = 2\n",
    "kmeans = KMeans(n_clusters = 2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dd1ee64-0a4d-4680-85fc-51620665c005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# возмем новые предложения\n",
    "prediction = ['Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.',\n",
    "              'Urusov set up the theatre in collaboration with English tightrope walker Michael Maddox.',\n",
    "              'Until the mid-1990s, most foreign operas were sung in Russian, but Italian and other languages have been heard more frequently on the Bolshoi stage in recent years.']\n",
    " \n",
    "# применим две модели, сначала создадим векторы новых предложений (tfIdfVectorizer.transform),\n",
    "# затем отнесем их к одному из кластеров (kmeans.predict)\n",
    "kmeans.predict(tfIdfVectorizer.transform(prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
